{"timestamp": 1661331767.098897, "stored_source_code": "# Add description here\n#\n# *Note:* You can open this file as a notebook (JupyterLab: right-click on it in the side bar -> Open With -> Notebook)\n# Uncomment the next two lines to enable auto reloading for imported modules\n# %load_ext autoreload\n# %autoreload 2\n# For more info, see:\n# https://docs.ploomber.io/en/latest/user-guide/faq_index.html#auto-reloading-code-in-jupyter\n# If this task has dependencies, declare them in the YAML spec and leave this\n# as None\n##NOTE necessary to ignore warnings for usable report.\nimport warnings\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom pprint import pprint\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import BayesianRidge, Lasso, ARDRegression, LassoCV\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVR\nfrom sklearn.tree import DecisionTreeRegressor\n\nwarnings.filterwarnings(\"ignore\")\nfrom hydro_timeseries.plotting import plot_corr_matrix, tsplot, trend_eval_plot, get_windows, plot_fea_importance\nfrom hydro_timeseries.util import load_timeseries_csv, add_mean_vars, OLS_analysis, smape, \\\n    get_sample_weights\nfrom sktime.forecasting.model_selection import ExpandingWindowSplitter\nimport numpy as np\n\nimport pandas as pd\n\nfrom hydro_timeseries.variables import Variables\nfrom datetime import timedelta\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nupstream = None\n\n# This is a placeholder, leave it as None\nproduct = None\n'''\nLoad data for 15 minute and daily.\n'''\ndata = load_timeseries_csv(upstream['run-tests']['data'])\ndata_daily = data.resample('D').mean()\ndata_hourly = data.resample('h').mean()\n\ndata = add_mean_vars(data)\ndata_daily = add_mean_vars(data_daily)\ndata_hourly = add_mean_vars(data_hourly)\nplots_path = Path(plots_path)\n#Meteo is the most recent. Thats why 15 minutes\ntrain_until = pd.to_datetime(valid_from) - timedelta(minutes=15)\ntrain = data[:train_until]\nval = data[valid_from:]\n\n# NOTE drop NaN Values for the last two days\nval = val.dropna(axis = 0)\n\ntrain_daily = data_daily[:train_until]\ntrain_hourly = data_hourly[:train_until]\n'''\nDROPPED VARS\n\nsoil_moisture and volumetric_soil_water - almost perfect correlation even on the 15 min steps\n- For simplicity and speed of computation, only soil_moisture is used further. \n'''\nplt.figure()\ntrain[['soil_moisture_mean', 'volumetric_soil_water_mean']].plot(subplots=True, sharex=True, figsize=(10,10), title='15 minute')\nplt.show()\n\ncorr_mat = train[Variables.soil_moisture_water].dropna().corr()\nplot_corr_matrix(corr_mat, '15 minute')\n\nprint(corr_mat)\n'''\nVar Selectors\n'''\nmeteo_sel = ['Value'] + Variables.meteo_i\nmeteo_mean_sel = ['Value'] + Variables.meteo_means_i\n'''\nMULTIPLOTS\n\nMultiplots - eyecheck for patterns between meteo and target\npositive correlations:\nSTRONG: rising temperature -> rising transpiration\nWEAK: higher snow_mean -> higher soil_moisture\n\nnegative correlations:\nmedium: drop in pressure -> high precip_mean (storm)\n\nTARGET:\nsoil moisture -> target\n'''\nplt.figure()\ntrain[meteo_mean_sel].plot(subplots=True, sharex=True, figsize=(10,10), title='15 minute')\nplt.savefig(plots_path / 'meteo_eye_plot.png')\nplt.show()\n'''\nDETRENDED TARGET\n- further, the features are analyzed in regards to detrended target with a specified detrender\ndetrenders = ['arima_current', 'ets', 'exp', 'stl']\n- add lagged detrended target variable (lag 2)\n'''\n\ndetrender = 'arima_current'\nfeatures = load_timeseries_csv(upstream['feature-manual']['data'])\n\ntrend_eval_plot(features['Value'], features[detrender], compare=True, label=f\"Detrender: {detrender}\",\n                filepath=plots_path / \"chosen_detr.png\")\n\nvalue_detr = (features['Value'] - features[detrender]).dropna()\n\ntsplot(value_detr, label =f'Detrended target with {detrender}', filepath=plots_path / \"detrended_target.png\")\n\n\n'''\nAdd detrended target\n- include daily mean of the detrended target lagged by two days. \n'''\nfeatures_new = features.assign(value_detr = value_detr)\nfeatures_new['value_detr_daily_lag2'] = features_new['value_detr'].resample('D').mean().shift(2)\nfeatures_new['value_detr_daily_lag2'] = features_new['value_detr_daily_lag2'].ffill()\n\ntarget = 'Value'\ntarget_new = 'value_detr'\ntarget_new_lag2 = 'value_detr_daily_lag2'\n'''\nNOTE careful about dropna\n'''\nfeatures_new[['value_detr', 'value_detr_daily_lag2']].tail(784).resample('D').mean()\n'''\nDrop zero Values?\n'''\n# features_new = features_new[features_new['Value'] != 0]\n\nfeatures_train = features_new[:train_until]\nfeatures_test = features_new[valid_from:].dropna()\ncorr_sel = ['value_detr'] + Variables.meteo\ncorr_sel_means = ['value_detr'] + Variables.meteo_means_i\n'''\nTrain Correlation matrices for 15 minute and daily\n- most significant soil_moisture, temperature\n- interestingly precip_mean is around 0 correlation with residuals after detrending\nMeteo stations strongly correlate - for simplicity means are used further\n- perhaps different precip_i measurements seem different enough to be used later.\n'''\ncorr_mat = features_train[corr_sel].dropna().corr()\nplot_corr_matrix(corr_mat, '15 minute')\n\ncorr_mat_means = features_train[corr_sel_means].dropna().corr()\nplot_corr_matrix(corr_mat_means, '15 minute meteo means')\n\ncorr_mat_daily_means = features_train[corr_sel_means].dropna().resample('D').mean().corr()\nplot_corr_matrix(corr_mat_daily_means, 'Daily meteo means')\n'''\ntop10 meteo vars by correlation\n'''\nprint(\"Top 10 meteo\")\nprint(np.abs(corr_mat['value_detr'][Variables.meteo_i]).sort_values(ascending=False).head(10))\n\n\n# print(\"Meteo means\")\n# print(np.abs(corr_mat_means['value_detr'][Variables.meteo_means_i]).sort_values(ascending=False))\n'''\nOLS analysis of meteo + sin/cos transformed datetime cyclicals\n- R squared around 0.039 :D -> need for some advanced features\n- Cyclicals are chosen to be week_of_year and month only to not lead into memorization during training\n- precip_mean with p-val = 0.1216, perhaps most of its power has been explained away by detrending.\nNOTES:\nMulticollinearity refers to independent variables that are correlated. \nThis problem can obscure the statistical significance of model terms, \nproduce imprecise coefficients, and make it more difficult to choose the correct model.\n\nMulticollinearity - some sort of regularization - ridge...\n\n'''\nOLS_analysis(target_new, Variables.meteo_means_i + Variables.cyclical + [target_new_lag2], features_train)\n'''sin better than cos for cyclicals'''\nOLS_analysis(target_new, Variables.cyclical, features_train)\n'''\nOLS analysis of lagged STL trend variables \n'''\nOLS_analysis(target_new, Variables.stl, features_train)\nOLS_analysis(target_new, Variables.meteo_means_i + Variables.cyclical + Variables.stl, features_train)\nfeature_cols_pred = Variables.meteo_means_i +\\\n               Variables.cyclical_sin + \\\n               Variables.stl + \\\n               Variables.meteo_i + \\\n               Variables.meteo_ewms_minute + \\\n               Variables.meteo_ewms_hourly + \\\n               Variables.lagged + \\\n               [target_new_lag2]\n\ntarget_sc = StandardScaler()\nfeature_sc = StandardScaler()\n\nfs_df_train = features_train[[target_new] + feature_cols_pred].dropna()\nfs_df_test = features_test[[target_new] + feature_cols_pred].dropna()\n\nX_train = pd.DataFrame(feature_sc.fit_transform(fs_df_train[feature_cols_pred]), index=fs_df_train.index, columns=fs_df_train[feature_cols_pred].columns)\ny_train = pd.DataFrame(target_sc.fit_transform(fs_df_train[[target_new]]), index=fs_df_train.index)\n\nX_test = pd.DataFrame(feature_sc.fit_transform(fs_df_test[feature_cols_pred]), index=fs_df_test.index, columns=fs_df_test[feature_cols_pred].columns)\ny_test = pd.DataFrame(target_sc.fit_transform(fs_df_test[[target_new]]), index=fs_df_test.index)\n\n'''\nVar selector \n- extree\n- lgbm\n- lasso - worked best\n'''\n\n# var_selector_model = ExtraTreesRegressor(n_estimators=50, random_state=random_seed)\n# var_selector_model = LGBMRegressor(n_estimators=50, random_state=random_seed)\nvar_selector_model = Lasso(alpha=0.1)\n\n\nweights = get_sample_weights(X_train, daily_discount_rate)\nvar_selector_model = var_selector_model.fit(X_train, y_train)\nvar_selector = SelectFromModel(var_selector_model, prefit=True)\nX_train_new = var_selector.transform(X_train)\n\n\n# plot_fea_importance(var_selector_model, X_train)\n\nselected_vars_ = X_train.columns[var_selector.get_support()].to_list()\n\n# selected_vars = selected_vars_\n\nselected_vars = [\n 'stl_seasonal_lag2',\n 'soil_moisture_index_75',\n 'soil_moisture_index_104',\n 'soil_moisture_index_134',\n 'daily_mean_precip_104_lag1',\n 'daily_mean_precip_104_lag2',\n 'daily_mean_Value_lag2',\n]\n\nprint(\"Selected vars\")\npprint(selected_vars)\n# plot_fea_importance(var_selector_model, X_train[selected_vars_])\n'''\nRolling weekly cross val - monthly test set\n- used as model selection\n- walk forward model checking - too expensive for more complex models\n\n#TODO GaussianProcessRegressor using gpytorch, scikit GP SIGKILLs when dataset > 10000\n'''\n\nmonthly = 96*30\nweekly = 96*7\nfh_monthly = list(range(1, monthly))\n\nX_all = features_new[[target_new] + feature_cols_pred].dropna()\ncv = ExpandingWindowSplitter(initial_window=int(len(X_all) * 0.7), step_length=weekly, fh=fh_monthly)\ntrain_windows, test_windows = get_windows(X_all, cv)\n\n\nmodels = {\n    'ridge': Pipeline([('regressor', BayesianRidge())]),\n    'lasso': Pipeline([\n        # ('poly', PolynomialFeatures(interaction_only=False)),\n        ('regressor', LassoCV(alphas=[1e-5, 1e-4, 1e-3, 1e-2, 1e-1]))\n    ]),\n\n    'svm': Pipeline([\n        ('regressor', LinearSVR(C=2.0, loss = 'squared_epsilon_insensitive'))\n    ]),\n    'ardr': Pipeline([\n        ('regressor', ARDRegression())\n    ]),\n    'nn': Pipeline([\n        ('pca', PCA(n_components=4)),\n        ('regressor', MLPRegressor(hidden_layer_sizes=(32), alpha=5.0, early_stopping=True, learning_rate='adaptive'))]),\n\n    # # 'gp': Pipeline([('selector', SelectFromModel(Lasso(alpha=0.05))),\n    # #                 ('regressor', GaussianProcessRegressor())]),\n\n    'dt': Pipeline([('regressor', DecisionTreeRegressor(max_depth = 15, random_state=random_seed))]),\n    'exttree': Pipeline([('regressor', ExtraTreesRegressor(n_estimators=30, max_depth=15, random_state=random_seed))]),\n    'lgbm': Pipeline([('regressor', LGBMRegressor(n_estimators=30, max_depth=15, random_state=random_seed))]),\n\n}\n\nsmapes = defaultdict(list)\n\nfor fold_id, (train_ids, test_ids) in enumerate(cv.split(X_all)):\n    X_train = X_all.iloc[train_ids][feature_cols_pred]\n    X_test = X_all.iloc[test_ids][feature_cols_pred]\n    y_train = X_all.iloc[train_ids][target_new]\n    y_test = X_all.iloc[test_ids][target_new]\n\n    X_train_scaled = pd.DataFrame(feature_sc.fit_transform(X_train), index=X_train.index,\n                           columns=X_train.columns)\n\n    y_train_scaled = pd.DataFrame(target_sc.fit_transform(y_train.values.reshape(-1,1)),\n                                  index=y_train.index)\n\n    X_test_scaled = pd.DataFrame(feature_sc.fit_transform(X_test), index=X_test.index,\n                                  columns=X_test.columns)\n\n    y_test_scaled = pd.DataFrame(target_sc.fit_transform(y_test.values.reshape(-1,1)),\n                                  index=y_test.index)\n\n\n    sample_weights = get_sample_weights(X_train, daily_discount_rate)\n    for key in models:\n        model = models[key]\n\n        if 'selector' not in model.named_steps.keys():\n            '''\n            If feature selection is not part of pipeline, use pre selected list.\n            '''\n            X_train_scaled = X_train_scaled[selected_vars]\n            X_test_scaled = X_test_scaled[selected_vars]\n\n\n        if key == 'nn' or key == 'ardr' or key == 'gp':\n            model.fit(X_train_scaled, y_train_scaled)\n        else:\n            model.fit(X_train_scaled, y_train_scaled, regressor__sample_weight=sample_weights)\n\n        y_pred = target_sc.inverse_transform(model.predict(X_test_scaled).reshape(-1, 1))\n        y_pred = pd.DataFrame(y_pred, index=y_test_scaled.index, columns=[target_new])\n        value_pred = features_new.loc[y_pred.index][detrender] + y_pred.loc[y_pred.index][target_new]\n        value_true = features_new.loc[y_pred.index]['Value']\n        smapes[key].append(smape(value_true, value_pred))\n\n    # add detrenders arima, autoets, stl for comparison\n    value_pred = features_new.loc[X_test.index]['arima_current']\n    value_true = features_new.loc[X_test.index]['Value']\n    smapes['arima_daily'].append(smape(value_true, value_pred))\n\n    value_pred = features_new.loc[X_test.index]['ets']\n    smapes['ets'].append(smape(value_true, value_pred))\n\n    value_pred = features_new.loc[X_test.index]['stl']\n    smapes['stl'].append(smape(value_true, value_pred))\n\nsmapes = pd.DataFrame().from_dict(smapes)\nbest_model_name = np.mean(smapes).idxmin()\n\nplt.figure(figsize=(10,5))\nsns.boxplot(data = smapes)\nax = plt.gca()\nax.set_title(f'SMAPE across n_folds={len(train_windows)}. Best: {best_model_name}, mean={np.mean(smapes[best_model_name]):.4f}')\nplt.savefig(plots_path / 'cv_manual.png')\nplt.show()\n\nprint(np.mean(smapes))\nlm = models[best_model_name].named_steps['regressor']\nimportance = lm.coef_\nplt.figure(figsize=(10,10))\nplt.barh([x for x in selected_vars], importance)\nplt.tight_layout()\nplt.savefig(plots_path / 'weights_weekly.png')\nplt.show()\n'''\nWalk forward evaluation \n- only performed with the chosen model\n- var selection is done in the previous step - not a good idea to do it every step. \n'''\nstep = 96\nfh = np.arange(97, 193)\ncv = ExpandingWindowSplitter(initial_window=len(X_all[:pd.to_datetime(valid_from) - timedelta(minutes=15)]), step_length=step, fh=fh)\ntrain_windows, test_windows = get_windows(X_all, cv)\n\nselected_vars = [\n 'sin_month',\n 'stl_seasonal_lag2',\n 'evapotranspiration_81',\n 'temperature_20',\n 'pressure_hpa_20',\n 'pressure_mean_hourly_ewm192',\n 'soil_moisture_index_104_hourly_ewm96',\n 'soil_moisture_index_104_hourly_ewm192',\n 'daily_mean_Value_lag2',\n 'daily_mean_precip_mean_lag1',\n 'daily_mean_precip_mean_lag2',\n 'value_detr_daily_lag2',\n]\n\nfinal = Pipeline([\n         ('scaler', StandardScaler()),\n         # ('poly', PolynomialFeatures(interaction_only=True)),\n         # ('selector', SelectFromModel(Lasso(alpha=0.05))),\n         ('regressor', BayesianRidge()),\n])\npreds = []\ntruth = []\nfor fold_id, (train_ids, test_ids) in enumerate(cv.split(X_all)):\n\n    if 'selector' not in final.named_steps.keys():\n        X_train = X_all.iloc[train_ids][feature_cols_pred][selected_vars]\n        X_test = X_all.iloc[test_ids][feature_cols_pred][selected_vars]\n    else:\n        X_train = X_all.iloc[train_ids][feature_cols_pred]\n        X_test = X_all.iloc[test_ids][feature_cols_pred]\n\n    y_train = X_all.iloc[train_ids][target_new]\n    y_test = X_all.iloc[test_ids][target_new]\n\n    sample_weights = get_sample_weights(X_train, 0.05)\n\n    final.fit(X_train, y_train, regressor__sample_weight=sample_weights)\n    y_pred = pd.DataFrame(final.predict(X_test), index=X_test.index, columns=[target_new])\n\n    value_pred = features_new.loc[X_test.index][detrender] + y_pred[target_new]\n    preds.append(value_pred)\n    truth.append(features_new.loc[X_test.index][target])\n\ny_pred = pd.concat(preds)\ny_true = pd.concat(truth)\n\nsmape_final = smape(y_true, y_pred)\nsmape_arima = smape(y_true, features_new.loc[y_pred.index]['arima_current'])\n\nprint(f\"smape_final = {smape_final:.2f}\")\ntrend_eval_plot(y_true, y_pred, compare=True, label=f\"arima-ridge-manual-features\", filepath=plots_path / 'manual_walkforward.png')\nlm = final.named_steps['regressor']\nimportance = lm.coef_\nplt.figure(figsize=(10,10))\nplt.barh([x for x in selected_vars], importance)\nplt.tight_layout()\nplt.savefig(plots_path / 'weights_walkforward.png')\nplt.show()\nprint(\"Last days evaluation. Green dashed predicted trend, red prediction, blue actual\")\ntsplot(y_true['2021-12-31'], y_pred=y_pred['2021-12-31'], y_detr=features['2021-12-31'][detrender],\n       label=f'Daily curve prediction for 2021-12-31 with arima-ridge-manual', do_plot_acf=False,\n       filepath=plots_path / 'last_day_31.png'\n       )\n\ntsplot(y_true['2022-01-01'], y_pred=y_pred['2022-01-01'], y_detr=features['2022-01-01'][detrender],\n       label=f'Daily curve prediction for 2022-01-01 with arima-ridge-manual', do_plot_acf=False,\n       filepath=plots_path / 'last_day_1.png'\n       )\n'''\nOutput prediction\n'''\nX = X_all[selected_vars]\ny = X_all[target_new]\nsample_weights = get_sample_weights(X_all, 0.05)\n\nfinal.fit(X, y, regressor__sample_weight=sample_weights)\n\nX_test = features_new['2022-01-02':'2022-01-03'][selected_vars]\ntrend_test = features_new['2022-01-02':'2022-01-03'][detrender]\ny_pred_test = pd.DataFrame(final.predict(X_test), index=X_test.index, columns=[target_new])\nvalue_pred = trend_test + y_pred_test[target_new]\ntsplot(value_pred['2022-01-02'], y_detr=trend_test['2022-01-02'], do_plot_acf=False,\n       label=\"Curve prediction for 2022-01-02, model: arima-ridge-manual\",\n       filepath=plots_path / 'pred_02.png'\n       )\n\ntsplot(value_pred['2022-01-03'], y_detr=trend_test['2022-01-03'], do_plot_acf=False,\n       label='Curve prediction for 2022-01-03, model: arima-ridge-manual',\n       filepath=plots_path / 'pred_03.png'\n       )\nvalue_pred = pd.DataFrame(value_pred, columns=['Value'])\nvalue_pred.to_csv(product['data'])", "params": {"random_seed": 1, "valid_from": "2021-11-01", "daily_discount_rate": 0.005, "plots_path": "/home/m/repo/hydro-power-prediction/plots"}}