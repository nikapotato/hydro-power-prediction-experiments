{"timestamp": 1661320616.314584, "stored_source_code": "# Add description here\n#\n# *Note:* You can open this file as a notebook (JupyterLab: right-click on it in the side bar -> Open With -> Notebook)\n# Uncomment the next two lines to enable auto reloading for imported modules\n# %load_ext autoreload\n# %autoreload 2\n# For more info, see:\n# https://docs.ploomber.io/en/latest/user-guide/faq_index.html#auto-reloading-code-in-jupyter\n# If this task has dependencies, declare them in the YAML spec and leave this\n# as None\nfrom datetime import timedelta\nfrom pathlib import Path\n\nimport numpy as np\nfrom darts.utils.statistics import plot_hist\n\nfrom darts.metrics import rmse, mape\n\nfrom darts.utils.timeseries_generation import datetime_attribute_timeseries\nfrom matplotlib import pyplot as plt\nfrom numpy import mean\nfrom pandas import DatetimeIndex\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport darts.timeseries\nimport pandas as pd\nimport torch\nfrom darts import TimeSeries\nfrom darts.dataprocessing.transformers import Scaler, InvertibleMapper\nfrom darts.models import TCNModel\nfrom pytorch_lightning.callbacks import EarlyStopping\nfrom sklearn.model_selection import TimeSeriesSplit\n\nfrom hydro_timeseries.darts_utils import backtest_minute_data, to_daily, to_hourly, ffill_to_minute, to_timestep\nfrom hydro_timeseries.pytorch_utils import RMSELoss, MSELoss, MAELoss, pl_trainer_kwargs\nfrom hydro_timeseries.util import load_timeseries_csv, add_mean_vars\nfrom hydro_timeseries.variables import Variables\nfrom darts.models import RegressionModel\nfrom sklearn.linear_model import BayesianRidge\n\nupstream = None\n\n# This is a placeholder, leave it as None\nproduct = None\n\ndata = load_timeseries_csv(upstream['run-tests']['data'])\ndata = add_mean_vars(data)\ndata = data.dropna(axis = 0)\ndata_hourly = data.resample('h').mean()\n\nsimulate_from = pd.to_datetime(simulate_from)\nvalid_from = pd.to_datetime(valid_from)\n\ntrain_until = valid_from - timedelta(hours=1)\nplots_path = Path(plots_path)\ndata_ts = TimeSeries.from_dataframe(data)\n'''\nThe serious splits\n'''\nplt.figure(100, figsize=(25, 5))\ndata_ts[:valid_from]['Value'].plot(label=\"Training\")\ndata_ts[valid_from:simulate_from]['Value'].plot(label=\"Validation\")\ndata_ts[simulate_from:]['Value'].plot(label=\"Simulation\")\nplt.show()\n'''\nTransforming the target\n- daily, hourly, to reduce variance\n- possibly filter with GP?\n'''\nplt.figure(100, figsize=(20, 5))\ndata_ts['Value'].plot(label=\"15 min\")\nffill_to_minute(to_hourly(data_ts['Value']), data_ts.time_index).plot(label=\"hourly\")\n# ffill_to_minute(to_timestep(data_ts['Value'], '12h', mean), data_ts.time_index).plot(label='4h')\nffill_to_minute(to_daily(data_ts['Value']), data_ts.time_index).plot(label=\"daily\")\nplt.tight_layout()\nplt.savefig(plots_path / 'target_time_avg.png')\nplt.show()\n'''\nMinMaxScaler may be used when the upper and lower boundaries are well known from domain knowledge.\nScaler(MinMaxScaler())\nScaler(StandardScaler())\n'''\nhourly = to_hourly(data_ts)\nmean_covs = hourly[Variables.meteo_means_i][Variables.rain_mean]\n'''\ncyclicals\n'''\nmean_covs = mean_covs.add_datetime_attribute(attribute='month', cyclic=True)\nmean_covs = mean_covs.add_datetime_attribute(attribute=\"day_of_year\", cyclic=True)\n'''\nScalers\n'''\ntarget_scaler = Scaler()\ncov_scaler = Scaler()\nhourly_value = target_scaler.fit_transform(hourly['Value'])\nhourly_covs = cov_scaler.fit_transform(mean_covs)\ntrain, val = hourly_value.split_before(valid_from)\ntrain_covs, val_covs = hourly_covs.split_before(valid_from)\nval, sim = val.split_before(simulate_from)\nval_covs, sim_covs = val_covs.split_before(simulate_from)\ntcn = TCNModel(\n    input_chunk_length=96,\n    output_chunk_length=48,\n    n_epochs=30,\n    dropout=0.1,\n    dilation_base=2,\n    weight_norm=True,\n    kernel_size=5,\n    num_filters=3,\n    random_state=random_seed,\n    loss_fn=MSELoss,\n    pl_trainer_kwargs=pl_trainer_kwargs,\n    save_checkpoints=True,\n    log_tensorboard=True,\n    model_name='tcn',\n)\ntcn.fit(\n    series=train['Value'],\n    past_covariates=train_covs,\n    val_series=val['Value'],\n    val_past_covariates=val_covs,\n    verbose=True,\n)\n# model_best = TCNModel.load_from_checkpoint('tcn', best=True)\n# backtest_minute_data(model_best,\n#                      hourly_value,\n#                      data_df=data,\n#                      valid_from=simulate_from,\n#                      past_covariates = hourly_covs,\n#                      last_points_only=False,\n#                      stride=24,\n#                      forecast_horizon=48,\n#                      scaler=target_scaler,\n#                      retrain=False,\n#                      )\n\n# from darts.utils.statistics import plot_residuals_analysis\n#\n# plot_residuals_analysis(model_best.residuals(series))\n#\n# # %%\n# raw_errors = model_best.backtest(\n#     hourly_value,\n#     past_covariates=hourly_covs,\n#     start=valid_from, forecast_horizon=48, metric=rmse, reduction=None, verbose=True, retrain=False\n# )\n# # %%\n# raw_errors = target_scaler.inverse_transform(np.array(raw_errors))\n# # %%\n#\n# plot_hist(\n#     raw_errors,\n#     bins=np.arange(0, max(raw_errors), 1),\n#     title=\"Individual backtest error scores (histogram)\",\n# )\n\n# # # %%\n# # val_squared = toDailyAverage.transform(data_ts['Value'])\n# # val_real = toDailyAverage.inverse_transform(val_squared['Value'])\n#\n# # %%\n# plt.figure(100, figsize=(25, 5))\n# val_real.plot()\n# plt.show()\n# #\n# # dailyAverage.plot()\n#\n# # %%\n# data_hourly = data.resample('h').mean()\n# data_hourly = TimeSeries.from_dataframe(data_hourly)\n#\n# value = data_hourly['Value']\n# cov = data_hourly[Variables.meteo_means_i + Variables.meteo]\n#\n# # %%\n# value_scaler = Scaler()\n# cov_scaler = Scaler()\n# train_value_sc = value_scaler.fit_transform(value[:train_until])\n# train_cov_sc = cov_scaler.fit_transform(cov[:train_until])\n#\n# valid_value_sc = value_scaler.fit_transform(value[valid_from:])\n# valid_cov_sc = cov_scaler.fit_transform(cov[valid_from:])\n#\n# value_sc = TimeSeries.concatenate(train_value_sc, valid_value_sc)\n# cov_sc = TimeSeries.concatenate(train_cov_sc, valid_cov_sc)\n# # %%\n# '''\n# Bayesian ridge\n# '''\n# rain_train = cov_sc[Variables.precip]\n# rain_all = cov_sc[Variables.precip]\n#\n# value_lags = list(range(-96, -48))\n# cov_lags = list(range(-6,1))\n# brr = RegressionModel(lags=value_lags, lags_future_covariates=[-6, 0], model=BayesianRidge(compute_score=True))\n#\n# brr.fit(\n#     train_value_sc, future_covariates=rain_train\n# )\n#\n# # %%\n#\n# day_ahead = brr.predict(\n#     series=train_value_sc,\n#     future_covariates=rain_all,\n#     n=48,\n# )\n#\n# day_ahead = value_scaler.inverse_transform(day_ahead)\n#\n# # %%\n#\n# backtest_ts, eval_ts, rmse_test, mae_test = backtest_minute_data(brr, value_sc,\n#                                    future_covariates=rain_all ,scaler = value_scaler,\n#                                    data_df=data, valid_from=valid_from, retrain=False,\n#                                    stride=24, forecast_horizon=48, last_points_only=False\n#                                    )\n#\n#\n# # %%\n# toDailyAverage = InvertibleMapper(\n#     fn=lambda timestamp, x: x / timestamp.days_in_month,\n#     inverse_fn=lambda timestamp, x: x * timestamp.days_in_month,\n# )\n#\n# square = InvertibleMapper(\n#     fn=lambda x: np.power(x, 2),\n#     inverse_fn=lambda x: np.sqrt(x),\n# )\n'''\nRNNs\n\nDO NOT DELETE\n'''\n# # %%\n# # %%\n# '''\n# Loss fns\n# - Noisy 15 min data, perhaps MAE?\n# '''\n# def MAELoss(yhat,y):\n#     # return torch.mean(torch.abs(yhat - y))\n#     return torch.mean(torch.abs(yhat[:, :24, :]-y[:, :24, :])) +\\\n#            2 * torch.mean(torch.abs(yhat[:, 24:, :]-y[:, 24:, :]))\n#\n#\n# def RMSELoss(yhat,y):\n#     return torch.sqrt(torch.mean((yhat-y)**2))\n#\n# def MSELoss(yhat,y):\n#     return torch.mean((yhat-y)**2)\n# my_stopper = EarlyStopping(\n#     monitor=\"val_loss\",\n#     patience=10,\n#     min_delta=0.01,\n#     mode='min',\n# )\n#\n# pl_trainer_kwargs={\"callbacks\": [my_stopper]}\n#\n# # %%\n# '''\n# This is a neural network model that uses an RNN encoder to encode fixed-length input chunks, and\n# a fully connected network to produce fixed-length outputs.\n# This model supports past covariates (known for `input_chunk_length` points before prediction time).\n# '''\n# block_rnn = BlockRNNModel(\n#                         model='GRU',\n#                         input_chunk_length=96,\n#                         output_chunk_length=48,\n#                         dropout=0.25,\n#                         n_rnn_layers=3,\n#                         loss_fn=MSELoss,\n#                         nr_epochs_val_period=10,\n#                         save_checkpoints=True,\n#                         log_tensorboard=True,\n#                         model_name='block_rnn',\n#                         force_reset=True,\n#                         pl_trainer_kwargs=pl_trainer_kwargs\n#                         )\n# # %%\n# rnn = RNNModel(input_chunk_length=48,\n#                 training_length=48,\n#                 model='GRU',\n#                 dropout=0.3,\n#                 n_rnn_layers=1,\n#                 loss_fn=MAELoss,\n#                 nr_epochs_val_period=10,\n#                 save_checkpoints=True,\n#                 log_tensorboard=True,\n#                 model_name='rnn',\n#                 force_reset=True,\n#                 pl_trainer_kwargs=pl_trainer_kwargs\n#                )\n#\n#\n# # %%\n# # split_from = pd.to_datetime('2021-09-01')\n# block_rnn.fit(\n#         series=train_value_sc[:split_from],\n#         val_series=train_value_sc[split_from:],\n#         epochs=50,\n#         verbose=True,\n#         )\n# # %%\n# # model = RNNModel.load_from_checkpoint('rnn', best=True)\n# model = BlockRNNModel.load_from_checkpoint('block_rnn', best=True)\n#\n# # %%\n# backtest_ts, rmse_test, mae_test = backtest_minute_data(block_rnn, value_sc, scaler = value_scaler,\n#                                    data_df=data, valid_from=valid_from, retrain=False,\n#                                    stride=24, forecast_horizon=48, last_points_only=False\n#                                    )", "params": {"random_seed": 1, "valid_from": "2021-10-01", "simulate_from": "2021-12-01", "plots_path": "/home/m/repo/hydro-power-prediction/plots"}}