{"timestamp": 1661327528.688383, "stored_source_code": "# Add description here\n#\n# *Note:* You can open this file as a notebook (JupyterLab: right-click on it in the side bar -> Open With -> Notebook)\n# Uncomment the next two lines to enable auto reloading for imported modules\n# %load_ext autoreload\n# %autoreload 2\n# For more info, see:\n# https://docs.ploomber.io/en/latest/user-guide/faq_index.html#auto-reloading-code-in-jupyter\n\nfrom datetime import timedelta\n# If this task has dependencies, declare them in the YAML spec and leave this\n# as None\nfrom pathlib import Path\n\nimport numpy as np\nimport statsmodels.api as sm\nfrom sktime.forecasting.ets import AutoETS\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\nfrom sktime.forecasting.model_selection import ExpandingWindowSplitter\nfrom sktime.forecasting.trend import STLForecaster\nfrom sktime.transformations.series.detrend import STLTransformer\n\nfrom hydro_timeseries.plotting import get_windows, tsplot, trend_eval_plot\nfrom hydro_timeseries.util import load_timeseries_csv, add_mean_vars, generate_cyclical_features, cv_evaluate, smape, \\\n    rmse, mae\nfrom hydro_timeseries.variables import Variables\n\nupstream = None\n\n# This is a placeholder, leave it as None\nproduct = None\narima_init_steps = None\nn_lags = None\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndata = load_timeseries_csv(upstream['run-tests']['data'])\ndata = add_mean_vars(data)\n\nplots_path = Path(plots_path)\n'''\nCyclical datetime features\n'''\ndata = (\n        data\n        # .assign(hour = data.index.hour)\n        # .assign(day_of_month = data.index.day)\n        .assign(month = data.index.month)\n        # .assign(day_of_week = data.index.dayofweek)\n        .assign(day_of_year = data.index.day_of_year)\n        .assign(week_of_year = data.index.week)\n        )\ndata = generate_cyclical_features(data, 'month', 12, 1)\ndata = generate_cyclical_features(data, 'week_of_year', 53, 1)\ndata = generate_cyclical_features(data, 'day_of_year', 366, 1)\n'''\nGet daily means \n'''\ndata_daily = data.resample('D').mean()\nvalue_daily = data.Value.resample('D').mean()\ndef generate_lagged_vars(df, n_lags, colname='Value', start_from=2):\n    # df_n = df.copy()\n    new_vars = []\n    for n in range(start_from, n_lags + 1):\n        new_var_name = f\"daily_mean_{colname}_lag{n}\"\n        df[new_var_name] = df[colname].shift(n)\n        new_vars.append(new_var_name)\n    return df, new_vars\n\ndef generate_ewms(df, colname='Value', span_list=[3, 7, 14, 30], shift=2, var_suffix='daily'):\n    # df_n = df.copy()\n    new_vars = []\n    for span in span_list:\n        new_var_name = f\"{colname}_{var_suffix}_ewm{span}\"\n        df[new_var_name] = df[colname].ewm(span=span).mean().shift(shift)\n        new_vars.append(new_var_name)\n    return df, new_vars\n'''\nLagged Value means\nlag has to be at least 2, because only the prev day is known when day ahead is being predicted\nmeteo can lag by 1. \n'''\ndata_daily, value_lag_vars = generate_lagged_vars(data_daily, colname='Value', n_lags=n_lags, start_from=2)\ndata_daily, precip_mean_vars = generate_lagged_vars(data_daily, colname='precip_mean', n_lags=n_lags, start_from=1)\ndata_daily, soil_moisture_mean_vars = generate_lagged_vars(data_daily, colname='soil_moisture_mean', n_lags=n_lags, start_from=1)\ndata_daily, snow_mean_vars = generate_lagged_vars(data_daily, colname='snow_mean', n_lags=n_lags, start_from=1)\n\nlagged_selector = []\nfor var in Variables.meteo_i:\n    data_daily, meteo_lagged_vars = generate_lagged_vars(data_daily, colname=var, n_lags=n_lags, start_from=1)\n    lagged_selector = lagged_selector + meteo_lagged_vars\n\nprint(\"First two should be NaN, last two should be fine\")\nprint(data_daily[['Value', 'daily_mean_Value_lag2']].head())\nprint(data_daily[['Value', 'daily_mean_Value_lag2']].tail())\nselector = lagged_selector + value_lag_vars + precip_mean_vars + soil_moisture_mean_vars + snow_mean_vars\ndata = data.join(data_daily[selector], how='left')\n# data_joined.loc[:, value_lag_vars] = data[value_lag_vars].ffill()\ndata.loc[:, selector] = data[selector].ffill()\nprint(\"Value lag2, Value and daily_mean_Value_lag2 should be equal\")\nprint(data_daily.loc['2022-01-01'].Value)\nprint(data.loc['2022-01-03'][['Value', 'daily_mean_Value_lag2']].head())\nprint(\"precip_mean lag1, precip_mean and daily_mean_precip_mean_lag1 should be equal\")\nprint(data_daily.loc['2022-01-02'].precip_mean)\nprint(data.loc['2022-01-03'][['precip_mean', 'daily_mean_precip_mean_lag1']].head())\n'''\nExp Weighted averages - target + meteo\n'''\n'''\nDaily - long timeframe week, month, 3 days, well suited for target - target known only day backwards.\n#TODO - minute steps until something\n'''\ndaily_spans = [3, 7, 14, 30, 60]\ndata_daily, daily_value_ewms = generate_ewms(data_daily, span_list=daily_spans, colname='Value', shift=2)\n\nselector_ewm = daily_value_ewms\n\ndata = data.join(data_daily[selector_ewm], how='left')\ndata.loc[:, selector_ewm] = data[selector_ewm].ffill()\nprint(data[daily_value_ewms].head())\nprint(data[daily_value_ewms].tail())\n'''\nHourly\n'precip_mean',\n 'snow_mean',\n 'pressure_mean',\n 'temperature_mean',\n 'soil_moisture_mean',\n 'evapotranspiration_mean'\n'''\nspans_hourly = [6, 9, 16, 32, 96, 192, 768, 1536]\ndata_hourly = data.resample('H').mean()\n\nhourly_var_names = []\nfor var in Variables.meteo_means_i + Variables.meteo_i:\n    print(var)\n    data_hourly, hourly_var_ewms = generate_ewms(data_hourly, colname=var, span_list=spans_hourly, var_suffix='hourly', shift=0)\n    hourly_var_names += hourly_var_ewms\n\ndata = data.join(data_hourly[hourly_var_names], how='left')\ndata.loc[:, hourly_var_names] = data[hourly_var_names].ffill()\nprint(data.filter(regex='precip.*hourly').head())\nprint(data.filter(regex='precip.*hourly').tail())\n'''\nMinute steps - short timeframes\nwell suited for weather, weather covariates are known all the way to the specific time step\n'''\nspans_minute = [4, 8, 16, 32, 64, 128, 256]\n\nminute_var_names = []\nfor var in Variables.meteo_means_i + Variables.meteo_i:\n    print(var)\n    data, minute_var_ewms = generate_ewms(data, colname=var, span_list=spans_minute, var_suffix='minute', shift=0)\n    minute_var_names += minute_var_ewms\n\nprint(data.filter(regex='precip.*minute').head())\nprint(data.filter(regex='precip.*minute').tail())\ny = data.Value.asfreq('15min').dropna()\nstep = 96\nfh = np.arange(97, 193)\ncv = ExpandingWindowSplitter(initial_window=step*(arima_init_steps+1), fh=fh, step_length=step)\n\nn_splits = cv.get_n_splits(y)\nprint(f\"Number of Folds = {n_splits}\")\n\ntrain_windows, test_windows = get_windows(y, cv)\n# plot_windows(y, train_windows, test_windows)\n\nprint(y[test_windows[-1]].index)\ny_pred, y_test, smape_test, rmse_test, mae_test, df = cv_evaluate(forecaster=AutoETS(trend='add', damped_trend=True), y=y, cv=cv, X=None)\n\nprint(f\"AutoETS as detrender n_val={len(y_pred)} SMAPE={smape_test:.4f} RMSE={rmse_test:.2f} MAE={mae_test:.2f}\")\nfh_2days = np.arange(1, 193)\nets_trend = AutoETS(trend='add', damped_trend=True)\nets_trend.fit(y, fh = fh_2days)\ny_pred_2days = ets_trend.predict(fh = fh_2days)\ndata = data.assign(ets=y_pred)\ndata.loc[y_pred_2days.index, 'ets'] = y_pred_2days\n\n# ExponentialSmoothing(trend=\"add\", damped_trend=True, remove_bias=True)\ny_pred, y_test, smape_test, rmse_test, mae_test, df = cv_evaluate(forecaster=ExponentialSmoothing(trend=\"add\", damped_trend=True, remove_bias=True), y=y, cv=cv, X=None)\nprint(f\"ExponentialSmoothing as detrender n_val={len(y_pred)} SMAPE={smape_test:.4f} RMSE={rmse_test:.2f} MAE={mae_test:.2f}\")\n\ndata = data.assign(exp=y_pred)\nfh_2days = np.arange(1, 193)\nexp_trend = ExponentialSmoothing(trend=\"add\", damped_trend=True, remove_bias=True)\nexp_trend.fit(y, fh = fh_2days)\ny_pred_2days = exp_trend.predict(fh = fh_2days)\ndata.loc[y_pred_2days.index, 'exp'] = y_pred_2days\n'''\nSTL daily forecaster\n'''\ny = data.Value.resample('D').mean().dropna()\nstep = 1\nfh = [2]\ncv = ExpandingWindowSplitter(initial_window=step*(arima_init_steps+1), fh=fh, step_length=step)\n\nn_splits = cv.get_n_splits(y)\nprint(f\"Number of Folds = {n_splits}\")\n\ntrain_windows, test_windows = get_windows(y, cv)\n# plot_windows(y, train_windows, test_windows)\n\nprint(y[test_windows[-1]].index)\ny_pred, y_test, smape_test, rmse_test, mae_test, df = cv_evaluate(forecaster=STLForecaster(sp = 7, robust=True), y=y, cv=cv, X=None)\nfh_2days = [1,2]\nstl_trend = STLForecaster(sp = 7, robust=True)\nstl_trend.fit(y, fh = fh_2days)\ny_pred_2days = stl_trend.predict(fh = fh_2days)\n\ndata = data.assign(stl=y_pred)\ndata.loc[y_pred_2days.index, 'stl'] = y_pred_2days\ndata['stl'] = data['stl'].ffill()\neval_df = data[['stl', 'Value']].dropna()\ny_true = eval_df['Value']\ny_pred = eval_df['stl']\nsmape_stl = smape(y_true, y_pred)\nrmse_stl = rmse(y_true, y_pred)\nmae_stl = mae(y_true, y_pred)\n\nprint(f\"STL(daily, period=7) as a detrender n_val={len(y_pred)} SMAPE={smape_stl:.4f} RMSE={rmse_stl:.2f} MAE={mae_stl:.2f}\")\n'''\nARIMA (2,1,1) \nSTL(sp=7) for daily\n'''\ndata['arima_prev'] = None # 1 step arima\ndata['arima_current'] = None # 2nd step arima\ndata['stl_trend_lag2'] = None\ndata['stl_seasonal_lag2'] = None\ndata['stl_resid_lag2'] = None\ntrain_until = value_daily.index[arima_init_steps]\nstart_from = train_until\nstl_transf = STLTransformer(sp = 7, robust=True, return_components = True)\n\nfor idx, date in enumerate(sorted(value_daily[(arima_init_steps+1):].index)):\n        date_ahead = date + timedelta(days=1)\n        date_before = date - timedelta(days=1)\n        print(f'Current day={date.date()} predicting day={date_ahead.date()} with last training date={train_until.date()}')\n        arima_daily = sm.tsa.arima.ARIMA(value_daily[:train_until], order=(2, 1, 1), trend=None).fit()\n        steps = arima_daily.forecast(steps = 2)\n\n        stl_df = STLTransformer(sp = 7, robust=True, return_components = True).fit_transform(value_daily[:train_until])\n\n        print(f'- STL lag2 trend={stl_df.loc[train_until].trend} seasonal={stl_df.loc[train_until].seasonal}')\n\n        if date_ahead <= value_daily.index[-1]:\n                print(f'-inserting arima forecasts, stl components, into {date_ahead.date()} trained on data until {train_until.date()}\\n')\n                data.loc[date_ahead, 'arima_prev'] = steps[0]\n                data.loc[date_ahead, 'arima_current'] = steps[1]\n\n                data.loc[date_ahead, 'stl_trend_lag2'] = stl_df.loc[train_until]['trend']\n                data.loc[date_ahead, 'stl_seasonal_lag2'] = stl_df.loc[train_until]['seasonal']\n                data.loc[date_ahead, 'stl_resid_lag2'] = stl_df.loc[train_until]['resid']\n\n                train_until = date\n        else:\n                print(f\"Last forward filled day at date={date.date()}\")\n'''\nNOTE: 2 step prediction\ndata at start_from should have arima none\ndata at start_from + 1 should have arima none as well \n'''\nassert data.loc[start_from].arima_current is None, \"Arima should be two step predicted\"\nassert data.loc[start_from + timedelta(days = 1)].arima_current is None, \"Arima should be two step predicted\"\n\nassert data.loc[start_from + timedelta(days = 2)].arima_current is not None, \"Arima should be calculated here\"\n'''\nForward fill both arimas\n'''\ndata.loc[:, ['arima_current', 'arima_prev']] = data[['arima_current', 'arima_prev']].ffill()\n'''\nForward fill daily stl lagged vars\n\n'''\ndata.loc[:, ['stl_trend_lag2', 'stl_seasonal_lag2', 'stl_resid_lag2']] = data[['stl_trend_lag2', 'stl_seasonal_lag2', 'stl_resid_lag2']].ffill()\n\nprint(data[['stl_trend_lag2', 'stl_seasonal_lag2', 'stl_resid_lag2']].loc[start_from + timedelta(days=2)].head())\n'''\nSanity checks\n'''\n\nprint(\"Values arima_current and arima_prev should be empty\")\nprint(data.loc[(start_from + timedelta(days = 1)).date().isoformat()][['arima_current', 'arima_prev']])\n\nprint(\"Values arima_current and arima_prev should be forward filled\")\nprint(data.loc[(start_from + timedelta(days = 2)).date().isoformat()][['arima_current', 'arima_prev']])\n\nprint(f\"STL values should start from {start_from + timedelta(days=2)}\")\nprint(data[['stl_trend_lag2', 'stl_seasonal_lag2', 'stl_resid_lag2']].loc[(start_from + timedelta(days=2)).date().isoformat()])\nprint(data[['stl_trend_lag2', 'stl_seasonal_lag2', 'stl_resid_lag2']].loc[(start_from + timedelta(days=10)).date().isoformat()])\n'''\nTrue daily plot\n'''\ntsplot(data_daily.Value.dropna(), label = \"True daily\", filepath=plots_path / 'daily.png')\n'''\nTrend forecasts comparison\n- daily forecasts are forward filled to 15 min steps.\n'''\ndata['Value_daily'] = value_daily\ndata['Value_daily'] = data['Value_daily'].ffill()\n\ntrend_eval_plot(data['Value'], data['Value_daily'], label=\"True daily  - trend forecast for 15 min steps\", filepath=plots_path / 'true_daily_detr.png')\ndel data['Value_daily'] #NOTE IMPORTANT\n\n\ntrend_eval_plot(data['Value'], data['arima_current'], label=\"Arima(daily, 2,1,1)  - trend forecast\", filepath=plots_path / 'arima_detr.png')\ntrend_eval_plot(data['Value'], data['ets'], label=\"AutoETS(15min) - trend forecast\", filepath=plots_path / 'ets_detr.png')\ntrend_eval_plot(data['Value'], data['exp'], label=\"ExponentialSmoothing(15min) - trend forecast\", filepath=plots_path / 'exp_detr.png')\ntrend_eval_plot(data['Value'], data['stl'], label=\"STL(daily, period=7) - trend forecast\", filepath=plots_path / 'stl_detr.png')\ntrend_eval_plot(data['Value'], data['arima_current'], label=\"Arima(daily, 2,1,1)  - trend forecast\", compare=True,\n                filepath=plots_path / 'arima_detr_comparison.png'\n                )\ndata.to_csv(product['data'])", "params": {"random_seed": 1, "valid_from": "2021-11-01", "n_lags": 7, "arima_init_steps": 7, "plots_path": "/home/m/repo/hydro-power-prediction/plots"}}